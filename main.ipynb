{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336862bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import eigs\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787f405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# Don't change this unless testing different dataset.\n",
    "adj_filename = 'data/PEMS04.csv'\n",
    "traffic_filename = 'data/PEMS04.npz'\n",
    "weather_filename = 'data/weather.npz'\n",
    "\n",
    "# Note that num_X is a MULTIPLE of the prediction window\n",
    "# num_hours only does what it says if pred_window is an hour\n",
    "# otherwise, num_hours = 3 and window_size = 24 means it's 3 2-hour periods\n",
    "num_hours = 2\n",
    "num_days = 2\n",
    "num_weeks = 2\n",
    "\n",
    "# measured in 5 minute intervals\n",
    "# the code is not built to handle unreasonable window sizes\n",
    "# please limit your window size to the scope of several hours at worst\n",
    "pred_window_size = 12\n",
    "\n",
    "# Depth for convolution - represents neighbor depth.\n",
    "# Keep low to avoid slow model. 3 should be a reasonable limit.\n",
    "K = 3\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "blocks = 1\n",
    "\n",
    "gcn_filters = 2\n",
    "t_filters = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eec2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# timestamps_per_hour is equivalent to prediction window size\n",
    "timestamps_per_week = 2016\n",
    "timestamps_per_day = 288\n",
    "\n",
    "# returns filepath for specific parameters\n",
    "def get_filepath(num_hours, num_days, num_weeks):\n",
    "    return f'data/packaged/{num_hours}_{num_days}_{num_weeks}.npz'\n",
    "    \n",
    "def get_required_diff(hours, days, weeks, pred_window_size):\n",
    "    '''\n",
    "    Returns the window size of one data point\n",
    "    This window will metaphorically slide across the dataset, generating data\n",
    "    '''\n",
    "    \n",
    "    # Typically you would expect the week to be the longest.\n",
    "    # This is designed to accomodate for faulty input.\n",
    "    \n",
    "    week_window_size = timestamps_per_week * weeks\n",
    "    day_window_size = timestamps_per_day * days\n",
    "    hour_window_size = pred_window_size * hours\n",
    "    return max(week_window_size, day_window_size, hour_window_size) + pred_window_size\n",
    "\n",
    "def generate_traffic_data(file, hours, days, weeks, pred_window_size):\n",
    "    '''\n",
    "    Returns four arrays of dims ([S, N, F, T_h], [S, N, F, T_d], [S, N, F, T_w], [S, N, T_p])\n",
    "        S   -> number of datapoints total\n",
    "        N   -> number of nodes. For PEMS04, this is 307\n",
    "        F   -> number of features per node. For PEMS04, this is 3\n",
    "        T_h -> timestamps for hourly channel\n",
    "        T_d -> timestamps for daily channel\n",
    "        T_w -> timestamps for weekly channel\n",
    "        T_p -> timestamps for prediction output\n",
    "    '''\n",
    "    \n",
    "    data = np.load(file)['data'] # [16992, 307, 3]\n",
    "    \n",
    "    # Get range of possible start/stops given the data\n",
    "    window_size = get_required_diff(hours, days, weeks, pred_window_size)\n",
    "    num_datapoints = data.shape[0] - window_size + 1\n",
    "    \n",
    "    print(f'Window size: {window_size}')\n",
    "    print(f'Generating {num_datapoints} datapoints')\n",
    "    \n",
    "    # output arrays\n",
    "    X_h = []\n",
    "    X_d = []\n",
    "    X_w = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(num_datapoints):\n",
    "        t_0 = i + window_size - pred_window_size\n",
    "        \n",
    "        temp_xh = []\n",
    "        temp_xd = []\n",
    "        temp_xw = []\n",
    "        temp_y = []\n",
    "        \n",
    "        # Hourly\n",
    "        for hour in range(hours, 0, -1):\n",
    "            start = t_0 - hour * pred_window_size\n",
    "            for j in range(pred_window_size):\n",
    "                temp_xh.append(data[start + j])\n",
    "        \n",
    "        # Daily\n",
    "        for day in range(days, 0, -1):\n",
    "            start = t_0 - day * timestamps_per_day\n",
    "            for j in range(pred_window_size):\n",
    "                temp_xd.append(data[start + j])\n",
    "            \n",
    "        # Weekly\n",
    "        for week in range(weeks, 0, -1):\n",
    "            start = t_0 - week * timestamps_per_week\n",
    "            for j in range(pred_window_size):\n",
    "                temp_xw.append(data[start + j])\n",
    "                \n",
    "        # Expected output\n",
    "        for j in range(pred_window_size):\n",
    "            temp_y.append(data[t_0 + j][:,1])\n",
    "            \n",
    "        # Reshape to fit data\n",
    "        if hours > 0:\n",
    "            temp_xh = np.array(temp_xh, dtype = np.float32).transpose(1, 2, 0)\n",
    "        if days > 0:\n",
    "            temp_xd = np.array(temp_xd, dtype = np.float32).transpose(1, 2, 0)\n",
    "        if weeks > 0:\n",
    "            temp_xw = np.array(temp_xw, dtype = np.float32).transpose(1, 2, 0)\n",
    "        temp_y = np.array(temp_y, dtype = np.float32).transpose(1, 0)\n",
    "        \n",
    "        X_h.append(temp_xh)\n",
    "        X_d.append(temp_xd)\n",
    "        X_w.append(temp_xw)\n",
    "        y.append(temp_y)\n",
    "        \n",
    "    X_h = np.array(X_h, dtype = np.float32)\n",
    "    X_d = np.array(X_d, dtype = np.float32)\n",
    "    X_w = np.array(X_w, dtype = np.float32)\n",
    "    y = np.array(y, dtype = np.float32)\n",
    "        \n",
    "    print(f'X_h size: {X_h.shape}')\n",
    "    print(f'X_d size: {X_d.shape}')\n",
    "    print(f'X_w size: {X_w.shape}')\n",
    "    print(f'y size: {y.shape}')\n",
    "    print('')\n",
    "    \n",
    "    return (X_h, X_d, X_w, y)\n",
    "    \n",
    "def generate_weather_data(file, hours, days, weeks, pred_window_size):\n",
    "    data = np.load(file)['data']\n",
    "    \n",
    "    # Get range of possible start/stops given the data\n",
    "    window_size = get_required_diff(hours, days, weeks, pred_window_size)\n",
    "    num_datapoints = data.shape[0] - window_size + 1\n",
    "    \n",
    "    print(f'Window size: {window_size}')\n",
    "    print(f'Generating {num_datapoints} datapoints')\n",
    "    \n",
    "    W = []\n",
    "    \n",
    "    for i in range(num_datapoints):\n",
    "        t_0 = i + window_size - pred_window_size\n",
    "\n",
    "        temp_w = []\n",
    "        for j in range(pred_window_size):\n",
    "            temp_w.append(data[t_0 + j])\n",
    "            \n",
    "        temp_w = np.array(temp_w, dtype = np.float32).transpose(1, 0)\n",
    "        W.append(temp_w)\n",
    "        \n",
    "    W = np.array(W, dtype = np.float32)\n",
    "    \n",
    "    print(f'W size: {W.shape}')\n",
    "    \n",
    "    return W\n",
    "\n",
    "def generate_adjacency_matrix(adj_file, data_file):\n",
    "        data = np.load(data_file)['data']\n",
    "        num_nodes = data.shape[1]\n",
    "        \n",
    "        print('Generating adjacency matrix for traffic nodes.')\n",
    "        print(f'Number of nodes: {num_nodes}')\n",
    "        \n",
    "        # Initialize adjacency matrix\n",
    "        A = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "        print(f'Shape of adjacency matrix: {A.shape}')\n",
    "        \n",
    "        # Fill in adjacency matrix\n",
    "        with open(adj_file, 'r') as f:\n",
    "            f.readline()\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if len(row) != 3:\n",
    "                    continue\n",
    "                i, j = int(row[0]), int(row[1])\n",
    "                A[i, j] = 1\n",
    "                \n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b3e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Adjacency matrix filename: {adj_filename}.')\n",
    "# print(f'Traffic data filename: {traffic_filename}.')\n",
    "# print(f'Weather data filename: {weather_filename}')\n",
    "# print('')\n",
    "\n",
    "# #================\n",
    "# # Retrieving data\n",
    "# #================\n",
    "\n",
    "# print(f'Predicting using information from {num_hours} hours, {num_days} days, and {num_weeks} weeks prior to prediction period.')\n",
    "\n",
    "# savefile = get_filepath(num_hours, num_days, num_weeks)\n",
    "\n",
    "# print(f'Data will be saved to {savefile}')\n",
    "# print('')\n",
    "\n",
    "# # Retrieve generic pool of traffic data\n",
    "# X_h, X_d, X_w, y = generate_traffic_data(traffic_filename, num_hours, num_days, num_weeks, pred_window_size)\n",
    "\n",
    "# print('Successfully generated traffic dataset')\n",
    "\n",
    "# # Retrieve weather data\n",
    "# W = generate_weather_data(weather_filename, num_hours, num_days, num_weeks, pred_window_size)\n",
    "\n",
    "# print('Successfully generated weather dataset')\n",
    "\n",
    "# # Retrieve adjacency matrix\n",
    "# A = generate_adjacency_matrix(adj_filename, traffic_filename)\n",
    "\n",
    "# print('Successfully generated adjacency matrix')\n",
    "\n",
    "# #============\n",
    "# # Saving data\n",
    "# #============\n",
    "\n",
    "# np.savez_compressed(savefile, hourly = X_h, daily = X_d, weekly = X_w, pred = y, weather = W, adj_mx = A)\n",
    "# print('Successfully saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494a4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_Laplacian(W):\n",
    "    D = np.diag(np.sum(W, axis = 1))\n",
    "    L = D - W\n",
    "    lambda_max = eigs(L, k = 1, which = 'LR')[0].real\n",
    "    return (2 * L) / lambda_max - np.identity(W.shape[0])\n",
    "    \n",
    "def cheb_polynomial(L_tilde, K):\n",
    "    N = L_tilde.shape[0]\n",
    "    cheb_polynomials = [np.identity(N), L_tilde.copy()]\n",
    "    for i in range(2, K):\n",
    "        cheb_polynomials.append(2 * L_tilde * cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n",
    "    return cheb_polynomials\n",
    "\n",
    "class TAT(nn.Module):\n",
    "    def __init__(self, inputs, vertices, timesteps):\n",
    "        super(TAT, self).__init__()\n",
    "        self.U1 = nn.Parameter(torch.FloatTensor(vertices))\n",
    "        self.U2 = nn.Parameter(torch.FloatTensor(inputs, vertices))\n",
    "        self.U3 = nn.Parameter(torch.FloatTensor(inputs))\n",
    "        self.be = nn.Parameter(torch.FloatTensor(1, timesteps, timesteps))\n",
    "        self.Ve = nn.Parameter(torch.FloatTensor(timesteps, timesteps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        inner = torch.matmul(x.permute(0, 3, 2, 1), self.U1)\n",
    "        lhs = torch.matmul(inner, self.U2)\n",
    "        rhs = torch.matmul(self.U3, x)\n",
    "        product = torch.matmul(lhs, rhs)\n",
    "        E = torch.matmul(self.Ve, torch.sigmoid(product + self.be))\n",
    "        E_normalized = F.softmax(E, dim=1)\n",
    "        return E_normalized\n",
    "\n",
    "class SAT(nn.Module):\n",
    "    def __init__(self, inputs, vertices, timesteps):\n",
    "        super(SAT, self).__init__()\n",
    "        self.W1 = nn.Parameter(torch.FloatTensor(timesteps))\n",
    "        self.W2 = nn.Parameter(torch.FloatTensor(inputs, timesteps))\n",
    "        self.W3 = nn.Parameter(torch.FloatTensor(inputs))\n",
    "        self.bs = nn.Parameter(torch.FloatTensor(1, vertices, vertices))\n",
    "        self.Vs = nn.Parameter(torch.FloatTensor(vertices, vertices))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lhs = torch.matmul(torch.matmul(x, self.W1), self.W2)\n",
    "        rhs = torch.matmul(self.W3, x).transpose(-1, -2)\n",
    "        product = torch.matmul(lhs, rhs)\n",
    "        S = torch.matmul(self.Vs, torch.sigmoid(product + self.bs))\n",
    "        S_normalized = F.softmax(S, dim=1)\n",
    "\n",
    "        return S_normalized\n",
    "\n",
    "class SAT_Conv(nn.Module):\n",
    "    def __init__(self, K, cheb_polynomials, inputs, outputs):\n",
    "        super(SAT_Conv, self).__init__()\n",
    "        self.K = K\n",
    "        self.cheb_polynomials = cheb_polynomials\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(inputs, outputs)) for _ in range(K)])\n",
    "        \n",
    "    def forward(self, x, sat):\n",
    "        batch_size, vertices, inputs, timesteps = x.shape\n",
    "\n",
    "        final_outputs = []\n",
    "\n",
    "        for t in range(timesteps):\n",
    "\n",
    "            graph_signal = x[:, :, :, t]\n",
    "            output = torch.zeros(batch_size, vertices, self.outputs)\n",
    "\n",
    "            for k in range(self.K):\n",
    "                T_k = self.cheb_polynomials[k]\n",
    "                T_k_with_at = T_k.mul(sat)\n",
    "                theta_k = self.Theta[k]\n",
    "                rhs = T_k_with_at.permute(0, 2, 1).matmul(graph_signal)\n",
    "                output = output + rhs.matmul(theta_k)\n",
    "\n",
    "            final_outputs.append(output.unsqueeze(-1))\n",
    "\n",
    "        return F.relu(torch.cat(final_outputs, dim=-1))\n",
    "\n",
    "class ASTGCN_Block(nn.Module):\n",
    "    def __init__(self, inputs, K, chev_filters, time_filters, strides, cheb_polynomials, vertices, timesteps):\n",
    "        super(ASTGCN_Block, self).__init__()\n",
    "        self.TAt = TAT(inputs, vertices, timesteps)\n",
    "        self.SAt = SAT(inputs, vertices, timesteps)\n",
    "        self.SAt_conv = SAT_Conv(K, cheb_polynomials, inputs, chev_filters)\n",
    "        self.t_conv = nn.Conv2d(chev_filters, time_filters, kernel_size = (1, 3), stride = (1, strides), padding = (0, 1))\n",
    "        self.residual_conv = nn.Conv2d(inputs, time_filters, kernel_size = (1, 1), stride = (1, strides))\n",
    "        self.ln = nn.LayerNorm(time_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, vertices, features, timesteps = x.shape\n",
    "        tat = self.TAt(x)\n",
    "        x_TAt = torch.matmul(x.reshape(batch_size, -1, timesteps), tat).reshape(batch_size, vertices, features, timesteps)\n",
    "        sat = self.SAt(x_TAt)\n",
    "        gcn = self.SAt_conv(x, sat)\n",
    "        t_conv_output = self.t_conv(gcn.permute(0, 2, 1, 3))\n",
    "        x_residual = self.residual_conv(x.permute(0, 2, 1, 3))\n",
    "        x_residual = self.ln(F.relu(x_residual + t_conv_output).permute(0, 3, 2, 1)).permute(0, 2, 3, 1)\n",
    "        return x_residual\n",
    "\n",
    "class ASTGCN_Module(nn.Module):\n",
    "    def __init__(self, blocks, traffic_features, K, chev_filters, time_filters, strides, cheb_polynomials, outputs, vertices):\n",
    "        super(ASTGCN_Module, self).__init__()\n",
    "\n",
    "        self.BlockList = nn.ModuleList([ASTGCN_Block(traffic_features, K, chev_filters, time_filters, strides, cheb_polynomials, vertices, strides * outputs)])\n",
    "\n",
    "        self.BlockList.extend([ASTGCN_Block(time_filters, K, chev_filters, time_filters, 1, cheb_polynomials, vertices, outputs) for _ in range(blocks - 1)])\n",
    "\n",
    "        self.final_conv = nn.Conv2d(outputs, outputs, kernel_size = (1, time_filters))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.BlockList:\n",
    "            x = block(x)\n",
    "        output = self.final_conv(x.permute(0, 3, 1, 2))[:, :, :, -1].permute(0, 2, 1)\n",
    "        return output\n",
    "\n",
    "class Weather(nn.Module):\n",
    "    def __init__(self, weather_features, outputs):\n",
    "        super(Weather, self).__init__()\n",
    "        self.layer = nn.Linear(weather_features, outputs)\n",
    "        \n",
    "    def forward(self, W):\n",
    "        W = W.permute(0, 2, 1)\n",
    "        W = self.layer(W)\n",
    "        W = W.permute(0, 2, 1)\n",
    "        return W\n",
    "    \n",
    "class ASTGCN(nn.Module):\n",
    "    def __init__(self, blocks, traffic_features, K, chev_filters, time_filters, cheb_polynomials, outputs, hours, days, weeks, vertices, weather_features):\n",
    "        super(ASTGCN, self).__init__()\n",
    "        self.hourly = ASTGCN_Module(blocks, traffic_features, K, chev_filters, time_filters, hours, cheb_polynomials, outputs, vertices)\n",
    "        self.daily = ASTGCN_Module(blocks, traffic_features, K, chev_filters, time_filters, days, cheb_polynomials, outputs, vertices)\n",
    "        self.weekly = ASTGCN_Module(blocks, traffic_features, K, chev_filters, time_filters, weeks, cheb_polynomials, outputs, vertices)\n",
    "        self.weather = Weather(weather_features, vertices)\n",
    "        self.W_h = nn.Parameter(torch.FloatTensor(vertices, outputs))\n",
    "        self.W_d = nn.Parameter(torch.FloatTensor(vertices, outputs))\n",
    "        self.W_w = nn.Parameter(torch.FloatTensor(vertices, outputs))\n",
    "        self.W_weather = nn.Parameter(torch.FloatTensor(vertices, outputs))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(vertices, outputs))\n",
    "        \n",
    "    def forward(self, X_h, X_d, X_w, W):\n",
    "        h_out = self.hourly(X_h)\n",
    "        d_out = self.daily(X_d)\n",
    "        w_out = self.weekly(X_w)\n",
    "        weather_out = self.weather(W)\n",
    "        fusion = self.W_h.mul(h_out) + self.W_d.mul(d_out) + self.W_w.mul(w_out) + self.W_weather.mul(weather_out) + self.b\n",
    "        \n",
    "        return fusion\n",
    "\n",
    "def get_model(blocks, traffic_features, K, gcn_filters, t_filters, pred_window, hours, days, weeks, vertices, weather_features, A):\n",
    "    L_tilde = scaled_Laplacian(A)\n",
    "    cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor) for i in cheb_polynomial(L_tilde, K)]\n",
    "    return ASTGCN(blocks, traffic_features, K, gcn_filters, t_filters, cheb_polynomials, pred_window, hours, days, weeks, vertices, weather_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8e60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, loss_function, optimizer):\n",
    "    mae = 0\n",
    "    rmse = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        print(f'Training batch {i + 1}', end  = '\\r')\n",
    "        train_xh, train_xd, train_xw, train_w, train_y = data\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = model(train_xh, train_xd, train_xw, train_w)\n",
    "        loss = torch.sqrt(loss_function(pred_y, train_y))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "        mae += mean_absolute_error(train_y.detach().numpy().reshape(-1, 1), pred_y.detach().numpy().reshape(-1, 1))\n",
    "        rmse += mean_squared_error(train_y.detach().numpy().reshape(-1, 1), pred_y.detach().numpy().reshape(-1, 1), squared = True)\n",
    "        \n",
    "    mae = mae / (i + 1)\n",
    "    rmse = rmse / (i + 1)\n",
    "    print(f'TRAINING: MAE = {mae} RMSE = {rmse}')\n",
    "    return\n",
    "\n",
    "def train_test(model, training_loader, testing_loader, num_epochs, lr):\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr =  0.01)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'BEGIN: Epoch {epoch + 1}')\n",
    "        \n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            else:\n",
    "                nn.init.uniform_(p)\n",
    "        \n",
    "        print('Done initializing parameters.')\n",
    "        \n",
    "        model.train(True)\n",
    "        \n",
    "        avg_loss = train_one_epoch(model, training_loader, loss_function, optimizer)\n",
    "        \n",
    "        model.train(False)\n",
    "        \n",
    "        t_loss = 0\n",
    "        \n",
    "        # Testing\n",
    "        mae = 0\n",
    "        rmse = 0\n",
    "        for i, data in enumerate(testing_loader):\n",
    "            print(f'Testing batch {i + 1}', end  = '\\r')\n",
    "            test_xh, test_xd, test_xw, test_w, test_y = data\n",
    "            pred_y = model(test_xh, test_xd, test_xw, test_w)\n",
    "            mae += mean_absolute_error(test_y.detach().numpy().reshape(-1, 1), pred_y.detach().numpy().reshape(-1, 1))\n",
    "            rmse += mean_squared_error(test_y.detach().numpy().reshape(-1, 1), pred_y.detach().numpy().reshape(-1, 1), squared = True)\n",
    "            print(test_xh.shape)\n",
    "            \n",
    "        mae = mae / (i + 1)\n",
    "        rmse = rmse / (i + 1)\n",
    "        print(f'EPOCH {epoch + 1}: MAE = {mae} RMSE = {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreiving data from data/packaged/2_2_2.npz\n",
      "Hourly traffic dims: (12949, 307, 3, 24)\n",
      "Daily traffic dims: (12949, 307, 3, 24)\n",
      "Weekly traffic dims: (12949, 307, 3, 24)\n",
      "Weather dims: (12949, 16, 12)\n",
      "Prediction dims: (12949, 307, 12)\n",
      "Adjacency matrix dims: (307, 307)\n",
      "Number of traffic features: 3\n",
      "Number of weather features: 16\n",
      "Number of vertices: 307\n",
      "\n",
      "Training size: 9711\n",
      "Testing size: 3238\n",
      "X_h\n",
      "X_d\n",
      "X_w\n",
      "W\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "savefile = get_filepath(num_hours, num_days, num_weeks)\n",
    "    \n",
    "print(f'Retreiving data from {savefile}')\n",
    "\n",
    "packaged_data = np.load(savefile)\n",
    "\n",
    "# X_h = torch.from_numpy(packaged_data['hourly'])\n",
    "# X_d = torch.from_numpy(packaged_data['daily'])\n",
    "# X_w = torch.from_numpy(packaged_data['weekly'])\n",
    "# W = torch.from_numpy(packaged_data['weather'])\n",
    "# y = torch.from_numpy(packaged_data['pred'])\n",
    "\n",
    "# A = torch.from_numpy(packaged_data['adj_mx'])\n",
    "\n",
    "X_h = np.copy(packaged_data['hourly'])\n",
    "X_d = np.copy(packaged_data['daily'])\n",
    "X_w = np.copy(packaged_data['weekly'])\n",
    "W = np.copy(packaged_data['weather'])\n",
    "y = np.copy(packaged_data['pred'])\n",
    "\n",
    "A = np.copy(packaged_data['adj_mx'])\n",
    "\n",
    "traffic_features = X_h.shape[2]\n",
    "weather_features = W.shape[1]\n",
    "vertices = y.shape[1]\n",
    "\n",
    "packaged_data = None\n",
    "\n",
    "print(f'Hourly traffic dims: {X_h.shape}')\n",
    "print(f'Daily traffic dims: {X_d.shape}')\n",
    "print(f'Weekly traffic dims: {X_w.shape}')\n",
    "print(f'Weather dims: {W.shape}')\n",
    "print(f'Prediction dims: {y.shape}')\n",
    "print(f'Adjacency matrix dims: {A.shape}')\n",
    "print(f'Number of traffic features: {traffic_features}')\n",
    "print(f'Number of weather features: {weather_features}')\n",
    "print(f'Number of vertices: {vertices}')\n",
    "print('')\n",
    "\n",
    "#===============\n",
    "# Data splitting\n",
    "#===============\n",
    "\n",
    "data_indices = list(range(y.shape[0]))\n",
    "\n",
    "train_ind, test_ind = train_test_split(data_indices)\n",
    "\n",
    "print(f'Training size: {len(train_ind)}')\n",
    "print(f'Testing size: {len(test_ind)}')\n",
    "\n",
    "print('X_h')\n",
    "train_xh = torch.from_numpy(np.copy(X_h[train_ind]))\n",
    "test_xh = torch.from_numpy(np.copy(X_h[test_ind]))\n",
    "X_h = None\n",
    "print('X_d')\n",
    "train_xd = torch.from_numpy(np.copy(X_d[train_ind]))\n",
    "test_xd = torch.from_numpy(np.copy(X_d[test_ind]))\n",
    "X_d = None\n",
    "print('X_w')\n",
    "train_xw = torch.from_numpy(np.copy(X_w[train_ind]))\n",
    "test_xw = torch.from_numpy(np.copy(X_w[test_ind]))\n",
    "X_w = None\n",
    "print('W')\n",
    "train_w = torch.from_numpy(np.copy(W[train_ind]))\n",
    "test_w = torch.from_numpy(np.copy(W[test_ind]))\n",
    "W = None\n",
    "print('y')\n",
    "train_y = torch.from_numpy(np.copy(y[train_ind]))\n",
    "test_y = torch.from_numpy(np.copy(y[test_ind]))\n",
    "y = None\n",
    "\n",
    "\n",
    "print(f'Training X_h shape: {train_xh.shape}')\n",
    "print(f'Testing X_h shape: {test_xh.shape}')\n",
    "print(f'Training X_d shape: {train_xd.shape}')\n",
    "print(f'Testing X_d shape: {test_xd.shape}')\n",
    "print(f'Training X_w shape: {train_xw.shape}')\n",
    "print(f'Testing X_w shape: {test_xw.shape}')\n",
    "print(f'Training W shape: {train_w.shape}')\n",
    "print(f'Testing W shape: {test_w.shape}')\n",
    "print(f'Training y shape: {train_y.shape}')\n",
    "print(f'Testing y shape: {test_y.shape}')\n",
    "print('')\n",
    "\n",
    "training_dataset = TensorDataset(train_xh, train_xd, train_xw, train_w, train_y)\n",
    "testing_dataset = TensorDataset(test_xh, test_xd, test_xw, test_w, test_y)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size = 12, shuffle = True)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size = 12)\n",
    "\n",
    "train_xh = None\n",
    "test_xh = None\n",
    "train_xd = None\n",
    "test_xd = None\n",
    "train_xw = None\n",
    "test_xw = None\n",
    "train_w = None\n",
    "test_w = None\n",
    "train_y = None\n",
    "test_y = None\n",
    "\n",
    "print(f'Training loader size: {len(training_loader)}')\n",
    "print(f'Testing loader size: {len(testing_loader)}')\n",
    "\n",
    "#===============\n",
    "# Model Creation\n",
    "#===============\n",
    "\n",
    "# TODO make model\n",
    "model = get_model(blocks, traffic_features, K, gcn_filters, t_filters, pred_window_size, num_hours, num_days, num_weeks, vertices, weather_features, A)\n",
    "# model = Weather(weather_features, pred_window_size)\n",
    "\n",
    "#===================\n",
    "# Training + Testing\n",
    "#===================\n",
    "train_test(model, training_loader, testing_loader, epochs, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc2b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
